---
title: "ECON4408 Problem Set - RMarkdown"
author: "Gregorios Raymond"
date: "28 February 2022"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
message = FALSE prevents messages that are generated by code from appearing in the finished file.
warning = FALSE prevents warnings that are generated by code from appearing in the finished.
fig.cap = "..." adds a caption to graphical results.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(stargazer)
library(magrittr)
```

# Part 1 - World Bank Development Report Data

## Q1. What are the mean and standard deviation of GDP per capita and infant mortality across countries? Compare the average male illiteracy rate to the female illiteracy rate. Why might they be different?

```{r}
wbdr <- read_dta("wbdr.dta")
wbdr <- wbdr[-c(198), ] # remove 'World'
```

*wbdrNA* is the  dataset after I removed missing values in 'ppp_gdp' variable
```{r}
wbdrNA <- wbdr
wbdrNA <- complete.cases(wbdr$ppp_gdp)
wbdrNA <- wbdr[wbdrNA, ]
```

```{r}
mean(wbdrNA$ppp_gdp)
sd(wbdrNA$ppp_gdp)
```
*wbdrNA2* is the  dataset after I removed missing values in 'infant_mort' variable
```{r}
wbdrNA2 <- wbdr
wbdrNA2 <- complete.cases(wbdr$infant_mort)
wbdrNA2 <- wbdr[wbdrNA2, ]
```

```{r}
mean(wbdrNA2$infant_mort)
sd(wbdrNA2$infant_mort)
```
NB: the mean and stadandard deviation of PPP GDP is interpreted in dollar terms; i.e, mean of PPP GDP is $12404.46. For infant mortality rate, it is interpreted as "per 1000 live births" (i.e., mean of infant mortality is 35.63 per 1000 live births).

```{r}
mean(wbdr$fem_lit, na.rm = TRUE)
mean(wbdr$male_lit, na.rm = TRUE)
```
The average illiteracy rate for **female** is  22.772 percent (77.228 - 100).
The average illiteracy rate for **male** is 14.418 percent (85.582 - 100).

The difference in male and female illiteracy rates on average may be attributed to parents being more inclined to provide education for their male children as they believe that male is more likely to provide higher returns to education than female. Since we are looking at the average rate, it is likely that the female illiteracy rate is biased upward due to the higher female literacy rate in rich countries.

For a summary of the variables:
```{r}
stargazer(as.data.frame(wbdr), type="text")
```
## Q2. Restrict your data set to countries for which we have GDP per capita. What are the mean, minimum, and maximum illiteracy rate, and infant mortality rate among the 50 richest countries? Among the 50 poorest countries? What does this tell you about the relationship between income and illiteracy, and income and mortality?

Back to using *wbdrNA* dataset.
But now I will introduce 2 new dataset, *wbdrNAasc* and *wbdrNAdesc*, where I arrange wbdrNA into ascending and descending order, respectively.

```{r}
wbdrNAasc <- arrange(wbdrNA, ppp_gdp)
wbdrNAdesc <- arrange(wbdrNA, desc(ppp_gdp))
```

I will also use `head()` function to look for the first 50 observations in *wbdrNAasc* and *wbdrNAdesc* to figure out the 50 poorest and 50 richest countries.

```{r}
wbdrNAasc <- head(wbdrNAasc, 50) # 50 poorest
wbdrNAdesc <- head(wbdrNAdesc, 50) # 50 richest
```

50 richest countries:
```{r}
stargazer(as.data.frame(wbdrNAdesc), keep = c(6, 7), median = TRUE, type="text")
```
50 poorest countries:
```{r}
stargazer(as.data.frame(wbdrNAasc), keep = c(6, 7), median = TRUE, type="text")
```
NB: For infant mortality rate, the values are interpreted as "per 1000 live births" (i.e., the mean of infant mortality rate in the 50 richest countries is is 8.52 per 1000 live births). Similarly, for total illiteracy rate, the values are interpreted as "of population age ≥ 15 years old" (i.e., the mean of total illiteracy rate in the 50 richest countries is 5.2 % of population age ≥ 15 years old)

There is a strong negative correlation between income and illiteracy rates and income and infant mortality rates. The 50 richest countries have much lower average illiteracy and infant mortality rate relative to the 50 poorest. The maximum infant mortality rate in the 50 richest, however, shows 123.6 per 1000 live births. This information may reflect the unequal income distribution in Equatorial Guinea. Interestingly, the illiteracy rate in 5 of the 50 poorest countries is below 15% of the total population age ≥ 15 years old. One possible explanation is that these countries either are modern welfare states or having received international aid to alleviate inequality.

## Q3. Now, find the median GDP per capita. How is it different from the mean GDP per capita from part (a)? Why might the mean and median be different? What information can we infer about the income distribution (inequality) from this?

```{r}
stargazer(as.data.frame(wbdrNA), median = TRUE, keep = c(5), type="text")
```
The median of PPP GDP is \$6,821.15 while the mean is \$12,404.46.

The mean of PPP GDP is much higher than the median. The difference may reflect the skewed distribution of GDP. This information can infer high cross-country income inequality. The value of mean GDP is higher than the median due to high-income countries in the distribution (right skewed). 

The right-skewed distribution can be shown in a histogram:
```{r}
hist(wbdrNA$ppp_gdp, xlab = "GDP per capita, PPP 2005", main = "Distribution of Cross-Country Income")
abline(v = rep(median(wbdrNA$ppp_gdp)), col = "red", )
abline(v = rep(mean(wbdrNA$ppp_gdp)), col = "blue")
```

The $\color{red}{\text{red}}$ line shows the median value, while the $\color{blue}{\text{blue}}$ line represents the mean.

Using the mean value of GDP would not inform the correct information about average income across countries since the data is skewed. The median value would more accurately represent the income of most of the countries in this dataset.

# Return to unrestricted wbdr dataset

## Q4. Regress the illiteracy rate on per capita GDP. Report the coefficient on per capita GDP and its standard error; what do they tell you? Is the sign of the coefficient what you expected? Explain briefly. What is the t-statistic for this coefficient, and what does it tell you? Interpret the 95% confidence interval.

```{r}
wbdr$tot_illit <- 100 - wbdr$tot_lit # create a variable for total illiteracy rate
model1_2 <- lm(tot_illit ~ ppp_gdp, wbdr)
stargazer(model1_2, type = "text", omit = c(2), omit.stat = c("adj.rsq", "ser"), covariate.labels = c("GDP PPP"), dep.var.labels = c("Illiteracy Rate"), add.lines = list(c("t-stat", "-5.535"), (c("95% Confidence Interval", "(-0.0009575, -0.0004533)"))))
```

An increase in GDP per capita by \$1000 is associated with a reduction in illiteracy rates by 0.7054 percentage points. The sign of the coefficient is as expected since higher GDP per capita is reflected upon more income to afford schooling/education, and therefore an increased inability to read and write.

The t-statistic is -5.53, which is higher than the t-critical values of 1.65 (10% significance), 1.96 (5% significance), 2.57 (1% significance). Therefore, we can reject the null hypothesis that the coefficient of GDP per capita is not statistically significantly different from 0.

The 95% confidence interval is a range of values (i.e., -0.0009575 to -0.0004533) that we can be 95% confident contains the true mean of the population when the experiment is conducted infinite times.

The 95% confidence interval can also be interpreted as a lower and upper bound of the GDP per capita coefficient estimates. In the lower (upper) bound estimate, a \$1000 increase in GDP per capita is associated with a reduction in illiteracy rate by 0.4533 (0.9575) percentage points.

## Q5. Regress the infant mortality rate on GDP per capita. Is the coefficient on per capita GDP significantly different from zero? How do you know? Interpret the coefficient in terms of a $1000 difference in per capita GDP.
```{r}
model2 <- lm(infant_mort ~ ppp_gdp, wbdr)
stargazer(model2, type="text",omit = c(2), omit.stat = c("adj.rsq", "ser"), covariate.labels = c("GDP PPP"), dep.var.labels = c("Infant Mortality Rate"), add.lines = list(c("t-stat", "-8.888")))
```

The GDP per capita coefficient is statistically significantly different from zero. This can be shown from the t-statistic (|-8.92| > 1.65; 1.96; 2.57).

The coefficient is -0.0013651, therefore an increase in GDP per capita of \$1000 is associated with a 1.37 decrease in infant mortality (per 1000 live birth).

## Q6. Regress the infant mortality rate on the illiteracy rate. Graph a scatter plot of the data as well as the regression line.

```{r}
model3_2 <- lm(infant_mort ~ tot_illit, wbdr)
stargazer(model3_2, type = "text", omit = c(2), omit.stat = c("adj.rsq", "ser"), covariate.labels = c("Illiteracy Rate"), dep.var.labels = c("Infant Mortality Rate"))
``` 
```{r, warning = FALSE, message = FALSE}
ggplot(data = wbdr, mapping = aes(x = tot_illit, y = infant_mort)) +
  theme_bw() + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE,) + 
  xlab('Illiteracy Rate') +
  ylab('Infant Mortality Rate') +
  ggtitle('Scatter Plot: Illiteracy on Infant Mortality') +
  theme(plot.title = element_text(hjust = 0.5))
```

## Q7. Using the results from above, what can we say about the causal relationship between illiteracy and income (GDP)?

It is difficult to infer a causal relationship between GDP and illiteracy rates due to endogeneity concerns at play. For example, the case for reverse causality is possible, where countries with lower illiteracy rates are less able to achieve a higher income level. However, from the plot, we can see that, on average, countries with high illiteracy rates also face high infant mortality rates, this informs the common trends in developing countries.

Omitted variables such as (economic and political) institutions in each country can affect both illiteracy rate and income. Political actors play a major role in policy implementation. Extractive political institutions may prefer to retain dominance by not providing essential public goods such as free reading materials. Similarly, authoritarian political regime can halt foreign investment (e.g., due to the high risk of expropriation). As a result, GDP is negatively impacted by extractive political institution. Therefore, difference in institutional factors may explain the differences in illiteracy rates, which is not accounted for in the above analysis.

A side note, however, illiteracy rate is below 15% in 5 of the 50 poorest country. This situation may imply that there may be government policies that play a major role in reducing illiteracy even in low income countries.

# Part 2. Effects of Deworming on Health and Schooling Participation
```{r}
worms <- read_dta("worms.dta")
stargazer(as.data.frame(worms), median = TRUE, type = "text")
```

## Q1. Miguel and Kremer randomize over schools (not individuals) and to introduce deworming drugs to randomly selected treatment group to estimate the effect of deworming on school attendance. Why randomize at the school level? Think what issues might arise in your evaluation if you randomize at the individual level.

Randomization was implemented at the school level to account for the presence of externalities coming from deworming. Individual-level randomization underestimates the benefits of deworming programs. Individual-level randomization within school fails to account for potential positive externalities for the control group from reduced disease transmission (i.e., worm load reductions). Further, if the externality benefits are realized, the outcome differences between the treatment and control groups will be understated. As a result, the treatment on the treated effect is underestimated/downward biased.

## Q2.
### i. How many observations are there per pupil?
2. Pupil that either took the pill in 1998 or 1999.

### ii. What percentage are boys?

```{r}
summary(worms$sex == 1)
```
15347 out of 29470 pupils (52%) are boys

### iii. What percentage of pupils took the deworming pill in 1998?
```{r}
summary(worms$pill98 == 1)
```
7025 out of 32826 (21.4%) pupils are treated in 1998

### iv. What percentage of schools was assigned deworming in 1998? Is this more or less than the percentage of pupils who actually took the pill in 1998?

```{r}
summary(worms$pupid & worms$wgrp == 1) # School assigned in group 1 (treated in 1998) = 11639
summary(worms$pupid & worms$wgrp == 2) # School assigned in group 2 (treated in 1999) = 11995
summary(worms$pupid & worms$wgrp == 3) # School assigned in group 3 (treated in 2001) = 11158
```
The percentage of schools assigned deworming in 1998 is 33.45% (11639/34792 students) for Group 1, 34.48% for Group 2 (11995/34792 students), 32.07% for Group 3 (11158/34792 students). 

The amount of student assigned to Group 1 (schools assigned treatment in 1998) is more than the percentage of pupils who actually took the pill in 1998 (21.4%)

## Q3. Using the data, find the difference in outcomes (school participation and worm infection rates) between
### i. Students that took the pill and students which did not in 1998. Is this a good estimate of the effect of taking the pill on school attendance? Why or why not?

```{r}
# Need to remove missing values
worms_na <- worms
worms_na <- complete.cases(worms$totpar98)
worms_na <- worms[worms_na,]

# use tapply() function to find the mean of total participation rate for pupils that took deworming pill in 1998
tapply(worms_na$totpar98, worms_na$pill98 == 1, mean)
# Now, we just need to subtract the mean
0.8765350 - 0.7455613 # equals 0.1309737

# similarly, now for worm infection rates
worms_na2 <- worms
worms_na2 <- complete.cases(worms$infect_early99)
worms_na2 <- worms[worms_na2,]
tapply(worms_na2$infect_early99, worms_na2$pill98 == 1, mean)
0.2464986 - 0.5086878 # equals -0.2621892
```

Students that took the pill are found to be, on average, having higher participation rates (13.1% higher) and lower infection rates (26.2% lower) relative to those who did not take the pill. However, this is not a good estimate of the average treatment effect since there may be differences in characteristics and attributes of the absent students that did not take the pill in 1998 that may be correlated with the outcomes. For example, the absent students may be more likely to be sick, hence could not come to the school to get dewormed. These differences may also understate the treatment effects if there are deworming treatment externalities across schools. The differences in attributes confound the estimates, therefore results are biased.

### ii. Students in treatment schools versus students not in treatment schools in 1998 (regardless of whether they actually took the pill). How does this compare to the answer in part (i) and what does it imply?

```{r}
# Since we are looking for the treatment effect of schools assigned in group 1, group 2 and 3 are the comparison (control) groups.

tapply(worms_na$totpar98, worms_na$wgrp == 1, mean)
tapply(worms_na$totpar98, worms_na$wgrp == 2 & 3, mean)
0.8301484 - 0.706113 # equals 0.1240354

tapply(worms_na2$infect_early99, worms_na2$wgrp == 1, mean)
tapply(worms_na2$infect_early99, worms_na2$wgrp == 2 & 3, mean)
0.2691415 - 0.5238745 # equals -0.254733
```

Students in Group 1 in 1998 have higher participation rates (12.4% higher) and lower infection rates (25.5% lower) on average relative to the students in the control groups. The difference in outcomes between groups is quite similar to the previous result, however, these estimates now account for externalities across schools. It would be better if we can look at the population density and whether there are other treated kids that live or interact around the control kids. These two variables affect disease transmission and infection, hence worm burdens. If students in the control groups live near many treated kids, then there is a high chance that the control kids are benefitting from treatment externalities, hence we may observe a higher externalities effect.

# Part 3. School Inputs: The Case of Flip Charts in Kenya

```{r}
prosp <- read_dta("prosp.dta") # for RCT analysis
retro <- read_dta("retro.dta") # for OLS analysis

# need to divide wallchar variable by 4 in retro.dta
retro$wc_r <- retro$wallchar/4
```

## Q1. Descriptive Statistics: Examine the data sets (use describe and summarize commands just to see what’s in the data sets.) What are the mean and standard deviation of the normalized test score (nmsc) in the retrospective data? In the prospective data? Is this what you expected? Note: When measuring test scores in the remainder of this problem, we will always be using normalized test score.

```{r}
stargazer(as.data.frame(retro), type = "text", keep = c("nmsc"))
stargazer(as.data.frame(prosp), type = "text", keep = c("nmsc"))
```

Is it expected? Yes, since the test score is normalized/standardized, it should have a normal (bell-shaped) distribution. Any variable that went through z-score normalization process, should have a mean of 0 and standard deviation of 1.

For an additional evidence, we can plot the `nmsc` variable
```{r nmsc histogram, message=FALSE, warning=FALSE}
retro %>%
  ggplot( aes(x=nmsc)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  xlab('Normalized Test Score') +
  ylab('Density') +
  ggtitle('Normalized Test Score Histogram - Retrospective') +
  theme(plot.title = element_text(hjust = 0.5))

prosp %>%
  ggplot( aes(x=nmsc)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  xlab('Normalized Test Score') +
  ylab('Density') +
  ggtitle("Normalized Test Score Histogram - Prospective") +
  theme(plot.title = element_text(hjust = 0.5))
```

## Q2. OLS. In this part, examine the effect of wall charts on test scores using the retrospective data.
### i. In an attempt to estimate the effect of wall charts on test scores, regress normalized test score on wc_r. Report and interpret the coefficient on wc_r.

```{r}
# create new dataset, retro2, that remove missing values in wc_r variable
retro2 <- retro
retro2 <- complete.cases(retro2$wc_r)
retro2 <- retro[retro2, ]

# regress normalized test score on wc_r
model4_3 <- lm(nmsc ~ wc_r, retro2)
stargazer(model4_3, type="text", omit = c(2), omit.stat = c("adj.rsq"), covariate.labels = c("Wall Charts"), dep.var.labels = c("Normalized Test Score"))

# if you want to look at t-values and p-values
summary(model4_3)
```

The coefficient above shows the result of simple regression to estimate the impact of wall charts without controlling for other educational inputs. The coefficient indicates that the provision of wall charts in school will increase student test scores (on subjects with available wall charts) by 20.6% of a standard deviation, a result that is significant at the 1% level.

### ii. Now regress the normalized test score on wc_r, controlling for whether the classroom is indoors, whether the roof leaks, blackboard condition, textbooks per pupil (use bkpup), desks per pupil, teacher training level, and class size (see variable labels in the dataset to determine the definitions of the variables). 

### Why might you want to use each of these controls? How does the coefficient on wc_r in this regression compare with the coefficient on wc_r in (i)? What do you conclude from this comparison?

```{r}
# I believe there are few coding errors in the dataset, both 'struct' and 'bboard' are a dummy variable, hence there should not be any non-integers. Therefore, I decided to replace these values into NAs
retro2$struct <- replace(retro2$struct, retro2$struct == 0.5, NA)
retro2$bboard <- replace(retro2$bboard, retro2$bboard == 0.5, NA)
retro2$bboard <- replace(retro2$bboard, retro2$bboard == 0.75, NA)

model5_4 <- lm(nmsc ~ wc_r + struct + rain + bboard + bkpup + deskp + stalvl + classsz, retro2)
stargazer(model5_4, type="text") 
```

Adding the control variables ensures that other educational inputs that may be correlated with both students' educational outcomes and the number of wall charts are accounted for. Otherwise, the result of providing wall charts to students’ test scores may be biased.

For example, if we don't control for teacher training level, we may find a positive correlation between wall charts and educational outcomes, but it is also likely that the results is driven by higher quality teacher. Moreover, higher quality teacher may be more likely to demand more wall charts to be distributed to their class. As a result, the effect of providing wall charts to student educational outcomes is overestimated.

The interpretation of multiple regression is quite different compare to simple regression. In this case, controlling for other educational inputs while holding these inputs constant, providing school additional wall charts increase the test scores by about 20% of a standard deviation. The coefficient is almost similar to part (i) even with the additional controls, and both are significant at the 1% level.

### iii. What could be problematic with using OLS regression like (i) or (ii) to estimate the effect of wall charts on test scores? Which way would you expect the coefficient on wc_r to be biased? Why?

The problem of retrospective studies (i.e., OLS regressions) is that there are observed educational inputs (e.g., wall charts) that may be correlated with omitted variables (that can be unobserved) that affect both educational outcomes and the number of wall charts even if control variables were added. Furthermore, there could be measurement error problems, where the values recorded in the data are not be perfectly precise (e.g., could be due to human error). Classical measurement error could result in "attenutation bias", or the beta estimate will be closer to 0 than the true beta parameter.

Parents' characteristics may come into play, some parents that care about their children's learning process may provide a better home environment for study and might impact the test score positively. Arguably, these parents are likely to be more involved in any of the school programs and may demand more wall charts to be provided to their kids' class. Since parent's characteristics is unobserved and omitted in the regression model, thus the estimates of educational inputs on outcomes may be overestimated.

Another example related to teacher quality is their characteristics, which may be unoberserved, hence not included in the model. If teachers are uninterested and unwilling to teach in a class with lower-performing students, and they may be less likely to demand more of and use the wall charts provided by the program, then the effects of providing wall charts to promote learning, hence improve test scores may be underestimated.

## Q3. Randomised evaluation. For this part, use the prospective data set, prosp.dta. Note that in this data set, wc measures whether or not the student is in a school that received a package of four wall charts (including a wall map), so that wc is a dummy variable.

```{r}
# now use the "prosp" dataset
# remove missing values from "score" and "nmsc"
prosp_na <- prosp
prosp_na <- complete.cases(prosp$nmsc)
prosp_na <- prosp[prosp_na, ]
```

### i. Calculate the mean test score for tests taken by students in schools which received wall charts, and the mean test score for tests taken by students in schools which did not receive wall charts. Take the difference of these conditional means. How do you interpret the difference? Note that you can also calculate the difference of these conditional means by regressing test score on wc; do this, and verify that the coefficient on wc equals the difference of conditional means.

```{r}
mean(prosp_na$nmsc[prosp_na$wc == 1]) - mean(prosp_na$nmsc[prosp_na$wc == 0])
# -0.00368964

# or we can run a regression
model7 <- lm(nmsc ~ wc, prosp_na)
stargazer(model7, type="text", omit = c(2), omit.stat = c("adj.rsq"), covariate.labels = c("Wall Charts"), dep.var.labels = c("Normalized Test Score"))

# both results are similar
```

There is hardly any difference in students' test scores on average between schools that received wall charts and schools that did not. Surprisingly, providing wall charts to schools resulted in small negative impacts on students' test scores (i.e., negative 0.4% of a standard deviation in test score) though it is not statistically significant.

### ii. Note that in this sample, geography (sub = “ghc”) is a wall-chart-related subject since the wall chart packet includes a wall map. Calculate the difference in mean test score, for tests in wall-chart-related subjects, between students in schools that received wall charts and students in schools that did not. In trying to estimate the effect of wall charts on test scores, why might you prefer this to the calculation in part (i)? Interpret the result.

```{r}
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "ghc"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "ghc"])
# -0.008952263
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "sca"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "sca"])
# 0.02668808
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "mat"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "mat"])
# -0.001233701
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "hsb"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "hsb"])
# -0.01177551
```

Looking at a specific subject may be better than calculating the average test scores on all subjects (as was conducted in the retrospective study, or result from part i). For example, wall charts for geography subjects may have more effective impacts, possibly due to the wall map that might pick the interest of students. One may want to examine the effects of different subject’s test scores to find out if there are heterogeneous impacts of subject-specific wall charts on educational outcomes.

However, similar to part (i), there is hardly any difference in test scores across four-wall charts related to subjects for schools that received wall charts and schools that did not. Notably, the only positive difference is achieved by providing a Science/Agriculture wall chart, though it is a very small difference (i.e., 2.7% of a standard deviation improvement in test score). Providing Health Science/Business Education-related wall charts in school, surprisingly, reduces test scores (in this subject) by 1.2% of a standard deviation relative to comparison schools.

Another thing to note, in the retrospective study, wall maps were not given to schools. Therefore, the provided wall charts are likely to only be relevant to Math, Science/Agriculture, and Home Science/Business Education. The prospective study, therefore, can examine the difference in test scores for Geography/History/Civics/Religious Education between treatment and control schools

### iii. Calculate the difference in mean test score, for tests in non-wallchart- related subjects, between students in schools that received wall charts and students in schools that did not. Why would you want to do this?

```{r}
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "eng"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "eng"])
# -0.001206595
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "kis"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "kis"])
# -0.0201652
mean(prosp_na$nmsc[prosp_na$wc == 1 & prosp_na$sub == "acm"]) - mean(prosp_na$nmsc[prosp_na$wc == 0 & prosp_na$sub == "acm"])
# -0.01151898
```

Assuming the wall charts related subjects do not affect the test scores in non-wallchart- related subjects, then there should not be any difference in test scores for non-wall chart- related subjects between schools that received wall charts and schools that didn’t. If there are significant differences, then the provision of wall charts-related subjects creates spillover effects. It might be justified to provide wall charts if externalities are found considering the low cost of provision. However, there is no difference in average test scores, for tests in non-wall chart- related subjects between treatment and control schools.

## Q4. Summing up. Compare the results from OLS and the randomized evaluation. How do they differ? Where do the differences come from? Which estimate do you prefer, and why?

Comparing results from retrospective and prospective studies show that the effect of providing wall-chart to schools is overestimated under OLS analysis. OLS analysis shows an increase of 20% of a standard deviation in test scores, a result that is statistically significant and significant in magnitude. However, the result from Randomized Controlled Trial shows the opposite, only an increase of 0.4% of a standard deviation in test scores and not statistically significant in any level.

Further, if we look at the adjusted R2 from the OLS analysis, it is only 0.022 (Q2 part ii) implying that only 2.2% of the variation in test score is explained by the wall charts and other control variables. Therefore, many important variables are omitted from the analysis.

The difference in results is likely attributed to omitted variable bias that was not accounted for in the retrospective study, as a result, the coefficient is overestimated. Schools provided with wall charts have other unobserved characteristics that increase test scores or differences in parents’ and students’ characteristics (e.g., support, effort) at the individual level may result in higher test scores.

In terms of preference, I prefer the prospective analysis or the results from RCT. The advantages of RCT is that, if randomized correctly, treatment and control groups should be similar on average, hence analysing the average treatment effect is highly possible. It is better to have a more accurate result though the effect is virtually none than observing a positive and significant coefficient but is clouded by bias.